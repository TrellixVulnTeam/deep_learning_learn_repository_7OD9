{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    def __init__(self,c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # 线路2\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')\n",
    "        # 线路3\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')\n",
    "        # 线路4\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1) # 在通道维上输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 构建GoogLeNet\n",
    "# 5 个模块 每个模块之间使用步幅为2的3*3最大池化层减小输出高宽\n",
    "# 第一模块使用64通道的7*7卷积层\n",
    "b1 = nn.Sequential()\n",
    "b1.add(nn.Conv2D(32, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 第二个模块使用两个卷积层， 首先是64通道的1*1卷积层， 然后是通道增大3倍的3*3卷积层 输出 192\n",
    "b2 = nn.Sequential()\n",
    "b2.add(nn.Conv2D(32, kernel_size=1 ,activation='relu'),\n",
    "       nn.Conv2D(96, kernel_size=3, padding=1, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 第三个模块 2个完整的Inception块， 输出通道数为 64+128+32+32 = 256 128+192+96+64=480\n",
    "# 2:4:1:1 1/2 1/12\n",
    "# 4:6:3:2 1/2 1/8\n",
    "b3 = nn.Sequential()\n",
    "b3.add(Inception(32, (48, 64), (8, 16), 16),\n",
    "       Inception(64,(64, 96), (16, 48), 32),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 第四个模块， 5个完整的Inception快\n",
    "b4 = nn.Sequential()\n",
    "b4.add(Inception(96, (48,64), (8,24), 48),\n",
    "       Inception(80, (56,112), (12,24), 32),\n",
    "       Inception(64, (64,128), (12,32), 32),\n",
    "       Inception(56, (72,144), (16,32), 32),\n",
    "       Inception(128, (80,160), (16,64), 64),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "b5 = nn.Sequential()\n",
    "b5.add(Inception(128, (80, 160), (16, 64), 64),\n",
    "       Inception(192, (96, 192), (24, 64), 64),\n",
    "       nn.GlobalAvgPool2D())\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0 output shape :\t  (1, 32, 24, 24)\n",
      "sequential1 output shape :\t  (1, 96, 12, 12)\n",
      "sequential2 output shape :\t  (1, 240, 6, 6)\n",
      "sequential3 output shape :\t  (1, 416, 3, 3)\n",
      "sequential4 output shape :\t  (1, 512, 1, 1)\n",
      "dense0 output shape :\t  (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name, 'output shape :\\t ', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.8813, train acc 0.288, test acc 0.583, time 194.0 sec\n",
      "epoch 2, loss 0.8816, train acc 0.649, test acc 0.679, time 189.2 sec\n",
      "epoch 3, loss 0.5581, train acc 0.791, test acc 0.824, time 189.5 sec\n",
      "epoch 4, loss 0.4470, train acc 0.834, test acc 0.825, time 191.4 sec\n",
      "epoch 5, loss 0.4055, train acc 0.850, test acc 0.847, time 189.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size, ctx = 0.1, 5, 128, d2l.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}